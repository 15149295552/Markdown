### day01

- numpy中具有广播机制：
  - 数组与数值进行计算，数组中的每个元素都与数值进行计算
  - 数组与数组进行计算，对应元素对应计算
- 一行一个样本，一列一个特征
- 列与列之间类型可以不同，同一列内类型必须相同
- 变维：
  - 视图变维，复制变维：不会修改原始数据的维度
    - 视图变维：数据共享
    - 复制变维：数据独立
  - 就地变维:直接修改原始数据的维度
- 轴向axis
  - 0垂直
  - 1水平



### day03

- 线性拆分

  - ```
    np.linspace(起始值，终止值，个数)
    
    等差数列
    包头也包尾
    ```






### day04

- 在训练数据中，有输出数据的为有监督学习
- 在训练数据中，没有输出数据的为无监督学习
- 连续数据
  - 在某个区间范围内，任意的数据可能都会出现
- 离散数据
  - 只有几个可选值
- 回归问题：
  - 预测值为连续数据
- 分类问题：
  - 预测值为离散值
- 特征归一化
  - 线性函数归一化（范围缩放 MinMaxSaclar）
    - 将每列的最大值和最小值设为相同的区间[0,1]
  - 零均值归一化（均值移除）
    - 每一列的均值变为0，标准差变为1
- np.where(条件，成立的返回值，不成立的返回值)



### day05

- 超平面：线性模型
- 矩阵相乘
  - A的所有行 * B的所有列，对应位置相乘之后再相加
  - A的列数 和 B的行数相等，才能相乘
  - 结果维度：（A的行数，B的列数）
- 超参数：
  - 在构建模型之前，需要提前设定一系列的参数，这些参数能够决定模型的精度
  - 学习率learning rate  控制梯度下降的速度
  - 这些参数的设定，大部分取决经验值
- w1(w)  权重
- w0(b)   偏置
- 回归问题的损失函数：均方误差mse



### day06

- 欠拟合：
  - 数据分布比较复杂，而模型选择的比较简单，模型没有足够的能力表达当前这组数据
- 过拟合
  - 数据分布比较简单，而模型选择的比较复杂，过于拟合训练集，导致测试集误差较大
- 训练集的数据，一定不能有顺序（随机）



### day07

#### 回归问题简单总结

- 预测值为连续值为回归问题
- 线性回归
  - 根据真实值与预测值的偏差，构建损失函数，使用梯度下降求损失函数的极小值
- 线性模型的变种模型
  - 正则化：防止过拟合
    - L1范数：所有系数的绝对值之和
    - L2范数：所有系数的平方之和
  - 损失函数 + L1范数---》Lasso回归
  - 损失函数 + L2范数---》岭回归
- 多项式回归
  - 在线性模型的基础上，增加高次项，提高模型复杂度，处理欠拟合
- 决策树回归
  - 如何选取最优分割特征
    - cart，做回归时，使用mse选取
  - 树何时停止分裂
- 集成学习
  - Boosting
    - Adaboost
    - GBDT
  - Bagging
    - 随机森林
- 损失函数：
  - 均方误差
- 评价指标
  - 平均绝对误差
  - 中位数绝对偏差
  - R2得分



#### 分类

- 分类问题的损失函数:交叉熵  cross_entropy
- 查准率： 对的个数  / 预测出来的个数
- 召回率:   对的个数  / 真实的样本个数
- 每个类别都有自己的查准率，召回率
- ID3决策树：
  - 最优分割特征：信息增益
- C4.5决策树：
  - 最优分割特征：增益率





### day08

- 验证曲线，一次只能找到一个参数
- 一组好的数据，可以构建好的模型。一组不好的数据，一定不能构建好的模型
- 当在做分类业务时，看看类别是否均衡
- 样本类别均衡化
  - 上采样
  - 下采样
  - 样本不够，权重来凑



### day09

#### 分类问题简单总结

- 逻辑回归
  - 逻辑函数sigmoid
- 决策树分类：
  - 信息熵：信息熵越大，数据越混乱，信息熵越小，数据越纯
  - 如何选取最优分割特征
    - ID3:信息增益
    - C4.5：增益率
    - CART：sklearn底层提供的树
      - 回归：均方误差
      - 分类：基尼系数
      - 二叉树
  - 树何时停止分裂
    - 所有特征及特征值使用完成，没有可分裂的，则自动停止分裂
    - 当前节点中，没有样本了，该节点停止分裂
    - 树达到了人为设定的最大深度
    - 当前节点样本数量 小于 最小样本分割数，则停止分裂
    - 最小样本需要数
- 支持向量机SVM
  - 寻找最优的分类超平面
    - 正确性
    - 安全性
    - 公平性
    - 简单
  - 线性可分与线性不可分
    - 线性核函数
    - 多项式核函数
    - 径向基核函数
- 朴素贝叶斯
  - 贝叶斯定理 +  条件独立假设
  - 当特征与特征之间独立性比较强，分类结果有保障
  - 当特征与特征之间关联性比较强，分类结果比较差
- 分类损失函数
  - 交叉熵
- 分类业务评估指标
  - 精度
  - 错误率
  - 查准率：对的个数 / 预测的个数
  - 召回率：对的个数 / 真实的样本个数
  - f1得分：   2\*查准率\*召回率 /  查准率+召回率
  - 每个类别都有自己的查准率召回率f1得分
  - 混淆矩阵：
    - 查准率： 主对角线的值 / 当前列的和
    - 召回率：主对角线的值  / 当前行的和
  - 分类报告

#### 深度学习

- 神经元的本质就是线性模型





### day10

- 在分类业务中，有几个类别，神经网络最后的输出层就有几个神经元
- 在分类业务中，最后一层（输出层），激活函数为Softmax，转成相对概率
- 全连接神经网络，只能接受一维数据（特征）
- 有几个卷积核，就有几个通道的输出（有几个特征图谱）
- 计算机视觉中：
  - 纯图像方式：Opencv   PIL
  - 深度学习
- 池化区域为2x2，步长为2，会在高和宽的方向缩小为原来的一半









