### day01

- numpy中具有广播机制：
  - 数组与数值进行计算，数组中的每个元素都与数值进行计算
  - 数组与数组进行计算，对应元素对应计算
- 一行一个样本，一列一个特征
- 列与列之间类型可以不同，同一列内类型必须相同
- 变维：
  - 视图变维，复制变维：不会修改原始数据的维度
    - 视图变维：数据共享
    - 复制变维：数据独立
  - 就地变维:直接修改原始数据的维度
- 轴向axis
  - 0垂直
  - 1水平



### day03

- 线性拆分

  - ```
    np.linspace(起始值，终止值，个数)
    
    等差数列
    包头也包尾
    ```






### day04

- 在训练数据中，有输出数据的为有监督学习
- 在训练数据中，没有输出数据的为无监督学习
- 连续数据
  - 在某个区间范围内，任意的数据可能都会出现
- 离散数据
  - 只有几个可选值
- 回归问题：
  - 预测值为连续数据
- 分类问题：
  - 预测值为离散值
- 特征归一化
  - 线性函数归一化（范围缩放 MinMaxSaclar）
    - 将每列的最大值和最小值设为相同的区间[0,1]
  - 零均值归一化（均值移除）
    - 每一列的均值变为0，标准差变为1
- np.where(条件，成立的返回值，不成立的返回值)



### day05

- 超平面：线性模型
- 矩阵相乘
  - A的所有行 * B的所有列，对应位置相乘之后再相加
  - A的列数 和 B的行数相等，才能相乘
  - 结果维度：（A的行数，B的列数）
- 超参数：
  - 在构建模型之前，需要提前设定一系列的参数，这些参数能够决定模型的精度
  - 学习率learning rate  控制梯度下降的速度
  - 这些参数的设定，大部分取决经验值
- w1(w)  权重
- w0(b)   偏置
- 回归问题的损失函数：均方误差mse



### day06

- 欠拟合：
  - 数据分布比较复杂，而模型选择的比较简单，模型没有足够的能力表达当前这组数据
- 过拟合
  - 数据分布比较简单，而模型选择的比较复杂，过于拟合训练集，导致测试集误差较大
- 训练集的数据，一定不能有顺序（随机）



### day07

#### 回归问题简单总结

- 预测值为连续值为回归问题
- 线性回归
  - 根据真实值与预测值的偏差，构建损失函数，使用梯度下降求损失函数的极小值
- 线性模型的变种模型
  - 正则化：防止过拟合
    - L1范数：所有系数的绝对值之和
    - L2范数：所有系数的平方之和
  - 损失函数 + L1范数---》Lasso回归
  - 损失函数 + L2范数---》岭回归
- 多项式回归
  - 在线性模型的基础上，增加高次项，提高模型复杂度，处理欠拟合
- 决策树回归
  - 如何选取最优分割特征
    - cart，做回归时，使用mse选取
  - 树何时停止分裂
- 集成学习
  - Boosting
    - Adaboost
    - GBDT
  - Bagging
    - 随机森林
- 损失函数：
  - 均方误差
- 评价指标
  - 平均绝对误差
  - 中位数绝对偏差
  - R2得分



#### 分类

- 分类问题的损失函数:交叉熵  cross_entropy
- 查准率： 对的个数  / 预测出来的个数
- 召回率:   对的个数  / 真实的样本个数
- 每个类别都有自己的查准率，召回率
- ID3决策树：
  - 最优分割特征：信息增益
- C4.5决策树：
  - 最优分割特征：增益率





### day08

- 验证曲线，一次只能找到一个参数
- 一组好的数据，可以构建好的模型。一组不好的数据，一定不能构建好的模型
- 当在做分类业务时，看看类别是否均衡
- 样本类别均衡化
  - 上采样
  - 下采样
  - 样本不够，权重来凑

















